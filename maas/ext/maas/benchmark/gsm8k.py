import asyncio
import threading
import time
import torch
import re
from typing import Any, Callable, Dict, List, Optional, Tuple, Literal

from tenacity import retry, retry_if_exception_type, stop_after_attempt, wait_fixed

from maas.ext.maas.benchmark.benchmark import BaseBenchmark
from maas.logs import logger
from maas.utils.sanitize import sanitize

class GSM8KBenchmark(BaseBenchmark):
    def __init__(self,
                name: str,
                file_path: str,
                log_path: str,
                batch_size: int,
                controller: torch.nn.Module,
                operator_embeddings: List[List[float]],
                optimizer: torch.optim.Optimizer,
                latency_weight: float = 0.1,
                use_latency: bool = True,
                token_weight: float = 0.00001,
                use_tokens: bool = False,
                virtual_token_rate: float = 50.0,
                use_critical_path: bool = True,
                parallel_execution: bool = True,
                normalize_rewards: bool = False,):
        super().__init__(name, file_path, log_path, batch_size, controller, operator_embeddings, optimizer, latency_weight, use_latency, token_weight, use_tokens, virtual_token_rate, use_critical_path, parallel_execution, normalize_rewards)

    def extract_number(self, text: str) -> Optional[float]:
        matches = re.findall(r"[-+]?\d+(?:,\d{3})*(?:\.\d+)?|\d+\.\d+", str(text))
        if matches:
            last_number = matches[-1].replace(",", "")
            try:
                return float(last_number)
            except ValueError:
                return None
        else:
            return None

    def calculate_score(self, expected_output: float, prediction: float) -> Tuple[float, float]:
        if prediction is None:
            return 0.0, prediction
        return 1.0 if abs(expected_output - prediction) <= 1e-6 else 0.0, prediction

    @retry(stop=stop_after_attempt(20), wait=wait_fixed(1), retry=retry_if_exception_type(Exception), reraise=True)
    async def _generate_output(self, graph, input_text):
        return await asyncio.wait_for(graph(input_text), timeout=1500)

    async def evaluate_problem(self, problem: dict, graph: Callable):
        import time
        input_text = problem["question"]
        expected_output = self.extract_number(problem["answer"])

        start_time = time.time()
        try:
            result = await self._generate_output(graph, input_text)
            # Unpack result: (output, cost, logprob, total_virtual_tokens, layer_operator_info)
            if len(result) == 5:
                output, cost, logprob, cp_token, layer_operator_info = result
            elif len(result) == 4:
                # Backward compatibility: old graphs return 4 values
                output, cost, logprob, layer_operator_info = result
                cp_token = 0.0
            else:
                output, cost, logprob = result[0], result[1], result[2]
                cp_token = 0.0
                layer_operator_info = None
            latency = time.time() - start_time  # Record latency immediately after graph execution

            if not output:
                raise ValueError("output is empty")

            predicted_number = self.extract_number(output)
            score, extracted_output = self.calculate_score(expected_output, predicted_number)

            if score == 0:
                self.log_mismatch(input_text, expected_output, output, extracted_output)

            return input_text, output, expected_output, score, cost, logprob, cp_token, latency, layer_operator_info

        except Exception as e:
            logger.info(f"Maximum retries reached. Skipping this sample. Error: {e}")
            latency = time.time() - start_time
            return input_text, str(e), expected_output, 0.0, 0.0, torch.tensor(0.0, dtype=torch.float32, device=self.device), 0.0, latency, None

    def get_result_columns(self) -> List[str]:
        return ["question", "prediction", "expected_output", "score", "cost", "logprob", "cp_token", "latency"]
